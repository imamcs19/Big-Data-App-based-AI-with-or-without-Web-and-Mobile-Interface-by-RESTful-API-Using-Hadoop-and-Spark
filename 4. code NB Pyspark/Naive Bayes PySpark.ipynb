{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 129
    },
    "colab_type": "code",
    "id": "Q5fKb9to6Odu",
    "outputId": "ff079848-defd-4d87-e8dd-1e7cc1a2c5df",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "ZN-tNHPJCwlP",
    "outputId": "7f845996-f7d2-495c-b02e-8cc148bbfa1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vtMr5qHpCL0O"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "qbEd3kFaBS_o",
    "outputId": "728e4c21-9ef9-41ee-cc19-a69f2121b72b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 146
    },
    "colab_type": "code",
    "id": "TpGdfJM8C39y",
    "outputId": "4519288e-e138-48fa-db38-7579670c6afc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin\t\t\t\t\t   etc\t  opt\t sys\n",
      "boot\t\t\t\t\t   home   proc\t tensorflow-2.1.0\n",
      "content\t\t\t\t\t   lib\t  root\t tmp\n",
      "datalab\t\t\t\t\t   lib32  run\t tools\n",
      "dev\t\t\t\t\t   lib64  sbin\t usr\n",
      "dlib-19.18.0-cp27-cp27mu-linux_x86_64.whl  media  srv\t var\n",
      "dlib-19.18.0-cp36-cp36m-linux_x86_64.whl   mnt\t  swift\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IeWyHi2bBTCB"
   },
   "outputs": [],
   "source": [
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget -q https://archive.apache.org/dist/spark/spark-2.4.1/spark-2.4.1-bin-hadoop2.7.tgz\n",
    "!tar xf spark-2.4.1-bin-hadoop2.7.tgz\n",
    "!pip install -q findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VLL0UMtWBTEl"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/spark-2.4.1-bin-hadoop2.7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4ggj9ByqBTG3"
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222
    },
    "colab_type": "code",
    "id": "hDw9skGnBTJv",
    "outputId": "5926fe0f-337a-4bfb-aa24-07ac448b24a4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://32dd8a8eb1c6:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fe8a4e4f0b8>"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "HMLqu85sBTMm",
    "outputId": "fde4b16a-dc1d-4e44-b5f5-fd5154572fd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "513 Jps\n",
      "421 SparkSubmit\n"
     ]
    }
   ],
   "source": [
    "!jps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0FeiSq66BTPx"
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B9kxzRJ3BpI1"
   },
   "source": [
    "**Naive Bayes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aPXPCzPoGv8x"
   },
   "source": [
    "Referensi: https://runawayhorse001.github.io/LearningApacheSpark/classification.html\n",
    "\n",
    "1. Menyiapkan spark context dan SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k9pqPxgBBTUl"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"PySpark Naive Bayes Classifier\") \\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6TtjR70PHJCt"
   },
   "source": [
    "2. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fqOGqbNpBTXI"
   },
   "outputs": [],
   "source": [
    "df = spark.read.format('com.databricks.spark.csv') \\\n",
    "            .options(header='true', inferschema='true') \\\n",
    "            .load(\"nursery.csv\",header=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "colab_type": "code",
    "id": "zeePPFR9BTZ9",
    "outputId": "80bdc885-ddd9-442e-b5d9-7e55a4e8386d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+--------+--------+----------+----------+-------------+-----------+---------+\n",
      "|parents|has_nurs|    form|children|   housing|   finance|       social|     health|    class|\n",
      "+-------+--------+--------+--------+----------+----------+-------------+-----------+---------+\n",
      "|  usual|  proper|complete|     one|convenient|convenient|      nonprob|recommended|recommend|\n",
      "|  usual|  proper|complete|     one|convenient|convenient|      nonprob|   priority| priority|\n",
      "|  usual|  proper|complete|     one|convenient|convenient|      nonprob|  not_recom|not_recom|\n",
      "|  usual|  proper|complete|     one|convenient|convenient|slightly_prob|recommended|recommend|\n",
      "|  usual|  proper|complete|     one|convenient|convenient|slightly_prob|   priority| priority|\n",
      "+-------+--------+--------+--------+----------+----------+-------------+-----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FZ_0SPKVHMfy"
   },
   "source": [
    "3. Fungsi untuk mengubah fitur kategorikal menjadi numerik dan disimpan dalam bentuk *Dense Vector*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lUDkW1rpBTcj"
   },
   "outputs": [],
   "source": [
    "def get_dummy(df,categoricalCols,continuousCols,labelCol):\n",
    "\n",
    "    from pyspark.ml import Pipeline\n",
    "    from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "    from pyspark.sql.functions import col\n",
    "\n",
    "    indexers = [ StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c))\n",
    "                 for c in categoricalCols ]\n",
    "\n",
    "    # default setting: dropLast=True\n",
    "    encoders = [ OneHotEncoder(inputCol=indexer.getOutputCol(),\n",
    "                 outputCol=\"{0}_encoded\".format(indexer.getOutputCol()))\n",
    "                 for indexer in indexers ]\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders]\n",
    "                                + continuousCols, outputCol=\"features\")\n",
    "\n",
    "    pipeline = Pipeline(stages=indexers + encoders + [assembler])\n",
    "\n",
    "    model=pipeline.fit(df)\n",
    "    data = model.transform(df)\n",
    "\n",
    "    data = data.withColumn('label',col(labelCol))\n",
    "\n",
    "    return data.select('features','label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AmPE8A5pBTe7"
   },
   "outputs": [],
   "source": [
    "categorical = ['parents','has_nurs','form','children','housing','finance','social','health']\n",
    "numeric = []\n",
    "class_output = 'class'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "colab_type": "code",
    "id": "oxzSyNV9EeGF",
    "outputId": "9223a6c6-bb6a-4af1-a364-c675fe585642"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|            features|    label|\n",
      "+--------------------+---------+\n",
      "|(19,[1,4,7,10,18]...|recommend|\n",
      "|(19,[1,4,7,10,17]...| priority|\n",
      "|(19,[1,4,7,10],[1...|not_recom|\n",
      "|(19,[1,4,7,10,16,...|recommend|\n",
      "|(19,[1,4,7,10,16,...| priority|\n",
      "+--------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = get_dummy(df,categorical,numeric,class_output)\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4m2FHV11Iza3"
   },
   "source": [
    "4. merubah label kategorikal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "colab_type": "code",
    "id": "4ppwIoQsEl6o",
    "outputId": "95c3f6d7-2625-436e-b30f-e547f8915bbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+------------+\n",
      "|            features|    label|indexedLabel|\n",
      "+--------------------+---------+------------+\n",
      "|(19,[1,4,7,10,18]...|recommend|         4.0|\n",
      "|(19,[1,4,7,10,17]...| priority|         1.0|\n",
      "|(19,[1,4,7,10],[1...|not_recom|         0.0|\n",
      "|(19,[1,4,7,10,16,...|recommend|         4.0|\n",
      "|(19,[1,4,7,10,16,...| priority|         1.0|\n",
      "+--------------------+---------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "# Index labels, adding metadata to the label column\n",
    "labelIndexer = StringIndexer(inputCol='label',\n",
    "                             outputCol='indexedLabel').fit(data)\n",
    "labelIndexer.transform(data).show(5, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nydONGsfJF0Z"
   },
   "source": [
    "5. fitur yang memiliki *unique value* lebih dari 4 akan dianggap sebagai fitur kontinu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "colab_type": "code",
    "id": "iyuLbjrfEo1j",
    "outputId": "0ed87ade-b097-49ea-c971-acbabe105b74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------------------+\n",
      "|            features|    label|     indexedFeatures|\n",
      "+--------------------+---------+--------------------+\n",
      "|(19,[1,4,7,10,18]...|recommend|(19,[1,4,7,10,18]...|\n",
      "|(19,[1,4,7,10,17]...| priority|(19,[1,4,7,10,17]...|\n",
      "|(19,[1,4,7,10],[1...|not_recom|(19,[1,4,7,10],[1...|\n",
      "|(19,[1,4,7,10,16,...|recommend|(19,[1,4,7,10,16,...|\n",
      "|(19,[1,4,7,10,16,...| priority|(19,[1,4,7,10,16,...|\n",
      "+--------------------+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorIndexer\n",
    "# Automatically identify categorical features, and index them.\n",
    "# Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer = VectorIndexer(inputCol=\"features\", \\\n",
    "                                  outputCol=\"indexedFeatures\", \\\n",
    "                                  maxCategories=5).fit(data)\n",
    "featureIndexer.transform(data).show(5, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uu15YBu9JiIE"
   },
   "source": [
    "6. split data training:data test dengan perbandingan 60:40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "colab_type": "code",
    "id": "bZBdfNdvEqxb",
    "outputId": "1a5ee4b0-bf5b-4135-ade0-c0fd918a1837"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------+---------+\n",
      "|features                                      |label    |\n",
      "+----------------------------------------------+---------+\n",
      "|(19,[],[])                                    |not_recom|\n",
      "|(19,[0],[1.0])                                |not_recom|\n",
      "|(19,[0,2],[1.0,1.0])                          |not_recom|\n",
      "|(19,[0,2,6,9,12],[1.0,1.0,1.0,1.0,1.0])       |not_recom|\n",
      "|(19,[0,2,6,9,12,14],[1.0,1.0,1.0,1.0,1.0,1.0])|not_recom|\n",
      "+----------------------------------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------------------------------------------------------------+----------+\n",
      "|features                                                    |label     |\n",
      "+------------------------------------------------------------+----------+\n",
      "|(19,[0,2,6],[1.0,1.0,1.0])                                  |not_recom |\n",
      "|(19,[0,2,6,9],[1.0,1.0,1.0,1.0])                            |not_recom |\n",
      "|(19,[0,2,6,9,12,14,15],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])       |not_recom |\n",
      "|(19,[0,2,6,9,12,14,15,17],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|spec_prior|\n",
      "|(19,[0,2,6,9,12,14,15,18],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|spec_prior|\n",
      "+------------------------------------------------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and test sets (40% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.6, 0.4])\n",
    "\n",
    "trainingData.show(5,False)\n",
    "testData.show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RaiUGo4lEvzN"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors # !!!!caution: not from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import IndexToString,StringIndexer, VectorIndexer\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AoDmaIhMJyGf"
   },
   "source": [
    "7. fit Naive Bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bzTPcpBcEv4x"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "nb = NaiveBayes(featuresCol='indexedFeatures', labelCol='indexedLabel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q5VZAJhZJ1g7"
   },
   "source": [
    "8. Arsitektur *pipeline*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3wCrdbNMEv8H"
   },
   "outputs": [],
   "source": [
    "# Convert indexed labels back to original labels.\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
    "                               labels=labelIndexer.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "98SUxvKMEv-w"
   },
   "outputs": [],
   "source": [
    "# Chain indexers and tree in a Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, nb,labelConverter])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3lLyFjF6EwBg"
   },
   "outputs": [],
   "source": [
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x5LfrQmkJ8Zk"
   },
   "source": [
    "9. Membuat prediksi menggunakan data uji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "colab_type": "code",
    "id": "Hvl9-mE_E5fr",
    "outputId": "91ae4664-4995-4f61-c462-631c08d0c389"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------+\n",
      "|            features|     label|predictedLabel|\n",
      "+--------------------+----------+--------------+\n",
      "|(19,[0,2,6],[1.0,...| not_recom|    spec_prior|\n",
      "|(19,[0,2,6,9],[1....| not_recom|     not_recom|\n",
      "|(19,[0,2,6,9,12,1...| not_recom|     not_recom|\n",
      "|(19,[0,2,6,9,12,1...|spec_prior|    spec_prior|\n",
      "|(19,[0,2,6,9,12,1...|spec_prior|    spec_prior|\n",
      "+--------------------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "# Select example rows to display.\n",
    "predictions.select(\"features\",\"label\",\"predictedLabel\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wegfpbpVKCg9"
   },
   "source": [
    "10. evaluasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "ZxhikIk4E5iq",
    "outputId": "3a106dc0-c461-40d0-9043-98c6ead34306"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.152129\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NaiveBayes",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
