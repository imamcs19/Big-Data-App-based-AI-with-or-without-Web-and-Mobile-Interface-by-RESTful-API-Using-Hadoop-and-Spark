{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir NBMapReduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MapReduce MeanMinMaksJava.ipynb\n",
      "NBMapReduce\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./NBMapReduce/NBCDriver.java\n"
     ]
    }
   ],
   "source": [
    "%%file ./NBMapReduce/NBCDriver.java\n",
    "import java.io.IOException;\n",
    "\n",
    "import org.apache.hadoop.conf.Configuration;\n",
    "import org.apache.hadoop.fs.Path;\n",
    "import org.apache.hadoop.io.IntWritable;\n",
    "import org.apache.hadoop.io.Text;\n",
    "import org.apache.hadoop.mapreduce.Job;\n",
    "import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n",
    "import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n",
    "\n",
    "public class NBCDriver {\n",
    "\tpublic static void main(String[] args) throws IOException, InterruptedException, ClassNotFoundException {\n",
    "\t\tConfiguration conf=new Configuration();\n",
    "\t\t// The test input for which you want to find the acitivity that the Person should be doing\n",
    "\t\tconf.set(\"test_input\", args[0]);\n",
    "\t\tJob job = new Job(conf);\n",
    "\t\tjob.setJarByClass(NBCDriver.class);\n",
    "\t\tjob.setJobName(\"Naive_Bayes_calssifier using Hadoop\");\n",
    "\t\tFileInputFormat.setInputPaths(job, new Path(args[1]));\n",
    "\t\tFileOutputFormat.setOutputPath(job, new Path(args[2]));\n",
    "\t\tjob.setMapperClass(NBCMap.class);\n",
    "\t\tjob.setReducerClass(NBCReduce.class);\n",
    "\t\tjob.setMapOutputKeyClass(IntWritable.class);\n",
    "\t\tjob.setMapOutputValueClass(Text.class);\n",
    "\t\tjob.setOutputKeyClass(IntWritable.class);\n",
    "\t\tjob.setOutputValueClass(Text.class);\n",
    "\t\tboolean success = job.waitForCompletion(true);\n",
    "\t\tSystem.exit(success ? 0 : 1);\n",
    "\t}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./NBMapReduce/NBCMap.java\n"
     ]
    }
   ],
   "source": [
    "%%file ./NBMapReduce/NBCMap.java\n",
    "import java.io.IOException;\n",
    "import java.util.HashMap;\n",
    "import java.util.Map.Entry;\n",
    "\n",
    "import org.apache.hadoop.io.IntWritable;\n",
    "import org.apache.hadoop.io.LongWritable;\n",
    "import org.apache.hadoop.io.Text;\n",
    "import org.apache.hadoop.mapreduce.Mapper;\n",
    "\n",
    "public class NBCMap extends Mapper<LongWritable, Text, IntWritable, Text>{\n",
    "\tpublic static String output_key;\n",
    "\tpublic static String[] test_input=null;\n",
    "\tpublic static int count=0;\n",
    "\tpublic static HashMap<String,Integer> inputs=new HashMap<String,Integer>();\n",
    "\tpublic static double output_value=Double.NEGATIVE_INFINITY;\n",
    "\tpublic static HashMap<String,Double> output= new HashMap<String,Double>();\n",
    "\tpublic static HashMap<String,Double> outcome_count= new HashMap<String,Double>();\n",
    "\tpublic static HashMap<String,Double> features_count= new HashMap<String,Double>();\n",
    "\tpublic void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n",
    "\t\tif(test_input==null)\n",
    "\t\t\ttest_input=context.getConfiguration().get(\"test_input\").split(\"\\\\,\");\n",
    "\t\tString[] input=value.toString().split(\"\\\\,\");\n",
    "\t\tfor(int j=0;j<input.length;j++){\n",
    "\t\t\tif(j==input.length-1){\n",
    "\t\t\t\tif(outcome_count.containsKey(input[j]))\n",
    "\t\t\t\t\toutcome_count.put(input[j], outcome_count.get(input[j])+1);\n",
    "\t\t\t\telse\n",
    "\t\t\t\t\toutcome_count.put(input[j], (double) 1);\n",
    "\t\t\t}\n",
    "\t\t\telse{\n",
    "\t\t\t\tif(input[j].contentEquals(test_input[j])){\n",
    "\t\t\t\t\tif(!inputs.containsKey(j+\",\"+input[j]))\n",
    "\t\t\t\t\t\tinputs.put(j+\",\"+input[j], 0);\n",
    "\t\t\t\t\tif(features_count.containsKey(j+\",\"+input[j]+\"|\"+input[input.length-1]))\n",
    "\t\t\t\t\t\tfeatures_count.put(j+\",\"+input[j]+\"|\"+input[input.length-1], features_count.get(j+\",\"+input[j]+\"|\"+input[input.length-1])+1);\n",
    "\t\t\t\t\telse\n",
    "\t\t\t\t\t\tfeatures_count.put(j+\",\"+input[j]+\"|\"+input[input.length-1], (double) 1);\n",
    "\t\t\t\t}\n",
    "\t\t\t}\n",
    "\t\t}\n",
    "\t\t++count;\n",
    "\t}\n",
    "\tpublic void cleanup(Context context) throws IOException, InterruptedException{\n",
    "\t\t\n",
    "\t\tfor(Entry<String,Double> o_c:outcome_count.entrySet()){\n",
    "\t\t\tString output_class=o_c.getKey();\n",
    "\t\t\tfor(Entry<String,Integer> i:inputs.entrySet()){\n",
    "\t\t\t\tif(!features_count.containsKey(i.getKey()+\"|\"+output_class))\n",
    "\t\t\t\t\tfeatures_count.put(i.getKey()+\"|\"+output_class, (double) 0);\n",
    "\t\t\t}\n",
    "\t\t\tdouble output_class_count=o_c.getValue();\n",
    "\t\t\tdouble probability=output_class_count/count;\n",
    "\t\t\tfor(Entry<String,Double> f_c:features_count.entrySet()){\n",
    "\t\t\t\tif(f_c.getKey().split(\"\\\\|\")[1].contentEquals(output_class))\n",
    "\t\t\t\t\tprobability=probability*(f_c.getValue()/output_class_count);\n",
    "\t\t\t}\n",
    "\t\t\toutput.put(output_class, probability);\n",
    "\t\t}\n",
    "\t\tfor(Entry<String,Double> o:output.entrySet()){\n",
    "\t\t\tif(o.getValue()>output_value){\n",
    "\t\t\t\toutput_value=o.getValue();\n",
    "\t\t\t\toutput_key=o.getKey();\n",
    "\t\t\t}\n",
    "\t\t}\n",
    "\t\tcontext.write(new IntWritable(1),new Text(output_key));\n",
    "\t}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./NBMapReduce/NBCReduce.java\n"
     ]
    }
   ],
   "source": [
    "%%file ./NBMapReduce/NBCReduce.java\n",
    "import java.io.IOException;\n",
    "import java.util.HashMap;\n",
    "import java.util.Map.Entry;\n",
    "\n",
    "import org.apache.hadoop.io.IntWritable;\n",
    "import org.apache.hadoop.io.Text;\n",
    "import org.apache.hadoop.mapreduce.Reducer;\n",
    "\n",
    "public class NBCReduce extends Reducer<IntWritable, Text, IntWritable, Text>{\n",
    "\tpublic void reduce(IntWritable key, Iterable<Text> values, Context context) throws IOException, InterruptedException{\n",
    "\t\tDouble out_value=Double.NEGATIVE_INFINITY;\n",
    "\t\tString out_key=null;\n",
    "\t\tHashMap<String,Integer> final_output=new HashMap<String,Integer>();\n",
    "\t\tfor(Text value:values){\n",
    "\t\t\tif(final_output.containsKey(value.toString()))\n",
    "\t\t\t\tfinal_output.put(value.toString(), final_output.get(value.toString())+1);\n",
    "\t\t\telse\n",
    "\t\t\t\tfinal_output.put(value.toString(), 1);\n",
    "\t\t}\n",
    "\t\tfor(Entry<String,Integer> output:final_output.entrySet()){\n",
    "\t\t\tif(output.getValue()>out_value){\n",
    "\t\t\t\tout_value=(double) output.getValue();\n",
    "\t\t\t\tout_key=output.getKey();\n",
    "\t\t\t}\n",
    "\t\t}\n",
    "\t\tcontext.write(null, new Text(out_key));\n",
    "\t}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set permission dari file *.java agar dapat dieksekusi\n",
    "!chmod u+x ./NBMapReduce/*.java"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\opt\\hadoop-2.7.3\n"
     ]
    }
   ],
   "source": [
    "!echo %HADOOP_HOME%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"./NBMapReduce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Imacho\\akodingDTSbatch2\\koding MapReduceJava\\NBMapReduce\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NBCDriver.class\n",
      "NBCDriver.java\n",
      "NBCMap.class\n",
      "NBCMap.java\n",
      "NBCReduce.class\n",
      "NBCReduce.java\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm *.class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NBCDriver.java\n",
      "NBCMap.java\n",
      "NBCReduce.java\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: NBCDriver.java uses or overrides a deprecated API.\n",
      "Note: Recompile with -Xlint:deprecation for details.\n"
     ]
    }
   ],
   "source": [
    "#jika compile di Win OS\n",
    "!javac -cp \"%HADOOP_HOME%\\share\\hadoop\\common\\*;%HADOOP_HOME%\\share\\hadoop\\mapreduce\\*\" *.java\n",
    "\n",
    "#jika compile di Linux OS\n",
    "#!javac -cp \"${HADOOP_HOME}/share/hadoop/common/*:${HADOOP_HOME}/share/hadoop/mapreduce/*\" *.java"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NBCDriver.class\n",
      "NBCDriver.java\n",
      "NBCMap.class\n",
      "NBCMap.java\n",
      "NBCReduce.class\n",
      "NBCReduce.java\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added manifest\n",
      "adding: NBCDriver.class(in = 1677) (out= 905)(deflated 46%)\n",
      "adding: NBCMap.class(in = 4556) (out= 1909)(deflated 58%)\n",
      "adding: NBCReduce.class(in = 2485) (out= 1099)(deflated 55%)\n"
     ]
    }
   ],
   "source": [
    "!jar cvf NBMapReduce.jar *.class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset.txt\n",
      "NBCDriver.class\n",
      "NBCDriver.java\n",
      "NBCMap.class\n",
      "NBCMap.java\n",
      "NBCReduce.class\n",
      "NBCReduce.java\n",
      "NBMapReduce - Copy.jar\n",
      "NBMapReduce.jar\n",
      "output\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\n",
      "drwxr-xr-x   - hue    supergroup          0 2019-09-04 03:20 input\n",
      "drwxr-xr-x   - hue    supergroup          0 2019-09-04 03:24 output\n",
      "drwxr-xr-x   - ImamCS supergroup          0 2019-10-10 23:47 user\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing dataset.txt\n"
     ]
    }
   ],
   "source": [
    "%%file dataset.txt\n",
    "Urgent,Yes,Yes,Party\n",
    "Urgent,No,Yes,Study\n",
    "Near,Yes,Yes,Party\n",
    "None,Yes,No,Party\n",
    "None,No,Yes,Pub\n",
    "None,Yes,No,Party\n",
    "Near,No,No,Study\n",
    "Near,No,Yes,TV\n",
    "Near,Yes,Yes,Party\n",
    "Urgent,No,No,Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset.txt\n",
      "NBCDriver.class\n",
      "NBCDriver.java\n",
      "NBCMap.class\n",
      "NBCMap.java\n",
      "NBCReduce.class\n",
      "NBCReduce.java\n",
      "NBMapReduce.jar\n",
      "output\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir ./user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir ./user/ImamCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir ./user/ImamCS/nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir ./user/ImamCS/nb/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -copyFromLocal ./dataset.txt ./user/ImamCS/nb/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rw-r--r--   1 ImamCS supergroup        192 2019-10-10 23:49 user/ImamCS/nb/input/dataset.txt\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls ./user/ImamCS/nb/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Urgent,Yes,Yes,Party\n",
      "Urgent,No,Yes,Study\n",
      "Near,Yes,Yes,Party\n",
      "None,Yes,No,Party\n",
      "None,No,Yes,Pub\n",
      "None,Yes,No,Party\n",
      "Near,No,No,Study\n",
      "Near,No,Yes,TV\n",
      "Near,Yes,Yes,Party\n",
      "Urgent,No,No,Study\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat ./user/ImamCS/nb/input/dataset.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19/10/11 01:37:33 INFO client.RMProxy: Connecting to ResourceManager at localhost/127.0.0.1:8050\n",
      "19/10/11 01:37:34 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n",
      "19/10/11 01:37:36 INFO input.FileInputFormat: Total input paths to process : 1\n",
      "19/10/11 01:37:36 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "19/10/11 01:37:37 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1570732148023_0003\n",
      "19/10/11 01:37:37 INFO impl.YarnClientImpl: Submitted application application_1570732148023_0003\n",
      "19/10/11 01:37:38 INFO mapreduce.Job: The url to track the job: http://XVisible-Teams:8088/proxy/application_1570732148023_0003/\n",
      "19/10/11 01:37:38 INFO mapreduce.Job: Running job: job_1570732148023_0003\n",
      "19/10/11 01:38:15 INFO mapreduce.Job: Job job_1570732148023_0003 running in uber mode : false\n",
      "19/10/11 01:38:15 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "19/10/11 01:38:34 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "19/10/11 01:38:51 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "19/10/11 01:39:02 INFO mapreduce.Job: Job job_1570732148023_0003 completed successfully\n",
      "19/10/11 01:39:03 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=18\n",
      "\t\tFILE: Number of bytes written=240657\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=323\n",
      "\t\tHDFS: Number of bytes written=6\n",
      "\t\tHDFS: Number of read operations=6\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=124544\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=120672\n",
      "\t\tTotal time spent by all map tasks (ms)=15568\n",
      "\t\tTotal time spent by all reduce tasks (ms)=15084\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=15568\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=15084\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=15941632\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=15446016\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=10\n",
      "\t\tMap output records=1\n",
      "\t\tMap output bytes=10\n",
      "\t\tMap output materialized bytes=18\n",
      "\t\tInput split bytes=131\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce shuffle bytes=18\n",
      "\t\tReduce input records=1\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=2\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=376\n",
      "\t\tCPU time spent (ms)=4527\n",
      "\t\tPhysical memory (bytes) snapshot=468930560\n",
      "\t\tVirtual memory (bytes) snapshot=751235072\n",
      "\t\tTotal committed heap usage (bytes)=414187520\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=192\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=6\n"
     ]
    }
   ],
   "source": [
    "!%HADOOP_HOME%/bin/hadoop jar NBMapReduce.jar NBCDriver \"Urgent,No,No\" \\\n",
    "\"./user/ImamCS/nb/input/dataset.txt\" \\\n",
    "\"./user/ImamCS/nb/output2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -cat ./user/ImamCS/nb/output2/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/thumbs-up.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
